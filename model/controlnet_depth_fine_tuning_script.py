# -*- coding: utf-8 -*-
"""ControlNet Depth Fine-tuning Script

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aOuS0Hg20JomfcKBB00USEvx6mmhkPNJ
"""

!pip install -U "huggingface_hub[cli]"
!huggingface-cli login

!nvidia-smi

import os
import math
import random
import logging
from pathlib import Path

import accelerate
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from tqdm.auto import tqdm # python liabrary used to create smart and customizable progress bar
from PIL import Image
import json
import numpy as np
import cv2


from transformers import AutoTokenizer
from transformers.models.clip.modeling_clip import CLIPTextModel
from transformers import CLIPFeatureExtractor # Import CLIPFeatureExtractor

from diffusers import AutoencoderKL, UNet2DConditionModel, ControlNetModel
from diffusers import DDPMScheduler, StableDiffusionControlNetPipeline
from diffusers.optimization import get_scheduler
from diffusers.utils import logging as diffusers_logging

project_path_root = "/content/drive/MyDrive/Data Seekho/Controlnet_Finetune_Project"
config = {
    "pretrained_model_name_or_path": "runwayml/stable-diffusion-v1-5",
    "controlnet_pretrained_model_name_or_path": "lllyasviel/sd-controlnet-depth",

    "train_data_dir": os.path.join(project_path_root, "images"),
    "annotation_data_dir": "/content/drive/MyDrive/annotation",
    "caption_file": os.path.join(project_path_root, "captioning", "metadata.json"),
    "output_dir": "/content/Output_ControlNet_Finetune",

    "resolution": 512,
    "train_batch_size": 1,
    "gradient_accumulation_steps": 4,
    "learning_rate": 1e-5,
    "lr_scheduler": "constant",
    "lr_warmup_steps": 500,
    "num_train_epochs": 3,
    "max_train_steps": None,
    "seed": 42,
    "mixed_precision": "fp16",
    "gradient_checkpointing": True,
    "enable_xformers_memory_efficient_attention": False,
    "checkpointing_steps":500,
}

logger = logging.getLogger(__name__)
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)
diffusers_logging.set_verbosity_info()

class ControlNetDepthDataset(Dataset):
    def __init__(self, config):
        self.config = config
        self.image_folder = Path(config["train_data_dir"])
        self.annotation_folder = Path(config["annotation_data_dir"])
        self.caption_file = Path(config["caption_file"])
        self.resolution = config["resolution"]
        self.tokenizer = AutoTokenizer.from_pretrained(config["pretrained_model_name_or_path"], subfolder="tokenizer" )

        with open(self.caption_file, 'r', encoding='utf-8') as f:
            self.caption_data = json.load(f)

        self.samples = []
        for entry in self.caption_data:
            original_image_path = self.image_folder / entry["file_name"]
            annotation_image_path = Path(entry["annotation_file"])
            if not annotation_image_path.is_relative_to(self.annotation_folder) and not annotation_image_path.is_absolute():
                 annotation_image_path = self.annotation_folder / annotation_image_path.name
            caption = entry["text"]

            if original_image_path.is_file() and annotation_image_path.is_file():
                self.samples.append({
                    "original_image_path": original_image_path,
                    "annotation_image_path": annotation_image_path,
                    "caption": caption
                })
            else:
                logger.warning(f"Skipping entry due to missing file: {original_image_path} or {annotation_image_path}")

        logger.info(f"Loaded {len(self.samples)} valid samples for training.")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]


        original_image = Image.open(sample["original_image_path"]).convert("RGB")
        annotation_image = Image.open(sample["annotation_image_path"]).convert("L")
        annotation_image = annotation_image.convert("RGB")

        original_image = original_image.resize((self.resolution, self.resolution), Image.LANCZOS)
        annotation_image = annotation_image.resize((self.resolution, self.resolution), Image.LANCZOS)

        caption_tokens = self.tokenizer(
            sample["caption"],
            max_length=self.tokenizer.model_max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        ).input_ids[0]

        original_image_tensor = torch.tensor(np.array(original_image)).permute(2, 0, 1).float() / 127.5 - 1.0
        annotation_image_tensor = torch.tensor(np.array(annotation_image)).permute(2, 0, 1).float() / 255.0

        return {
            "input_ids": caption_tokens,
            "pixel_values": original_image_tensor,
            "conditioning_pixel_values": annotation_image_tensor,
        }

def train_controlnet_depth(config, resume_from_checkpoint=None):
    accelerator = accelerate.Accelerator(
        mixed_precision=config["mixed_precision"],
        gradient_accumulation_steps=config["gradient_accumulation_steps"],
        log_with="tensorboard",
        project_dir=config["output_dir"],
    )

    logger.info("Loading Stable Diffusion and ControlNet models...")

    vae = AutoencoderKL.from_pretrained(config["pretrained_model_name_or_path"], subfolder="vae")
    tokenizer = AutoTokenizer.from_pretrained(config["pretrained_model_name_or_path"], subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(config["pretrained_model_name_or_path"], subfolder="text_encoder")
    unet = UNet2DConditionModel.from_pretrained( config["pretrained_model_name_or_path"], subfolder="unet")
    if config["controlnet_pretrained_model_name_or_path"]:
        controlnet = ControlNetModel.from_pretrained(config["controlnet_pretrained_model_name_or_path"] )
    else:
        controlnet = ControlNetModel.from_unet(unet)

    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    unet.requires_grad_(False)

    dtype = torch.float16 if config["mixed_precision"] == "fp16" else torch.float32
    vae.to(accelerator.device, dtype=vae.dtype) # Ensure VAE is on device with its dtype
    text_encoder.to(accelerator.device, dtype=text_encoder.dtype) # Ensure text_encoder is on device with its dtype
    unet.to(accelerator.device, dtype=unet.dtype) # Ensure unet is on device with its dtype
    controlnet.to(accelerator.device, dtype=controlnet.dtype) # Ensure controlnet is on device with its dtype


    #if config["enable_xformers_memory_efficient_attention"]:
     #   vae.enable_xformers_memory_efficient_attention()
      #  unet.enable_xformers_memory_efficient_attention()
       # controlnet.enable_xformers_memory_efficient_attention()

    if config["gradient_checkpointing"]:
        controlnet.enable_gradient_checkpointing()

    optimizer = torch.optim.AdamW(
        controlnet.parameters(),
        lr=config["learning_rate"],
        betas=(0.9, 0.999),
        weight_decay=1e-2,
        eps=1e-08,
    )

    train_dataset = ControlNetDepthDataset(config)
    train_dataloader = DataLoader(
        train_dataset,
        shuffle=True,
        batch_size=config["train_batch_size"],
        num_workers=os.cpu_count() // 2 if os.cpu_count() else 0,
    )

    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / config["gradient_accumulation_steps"])
    if config["max_train_steps"] is None:
        config["max_train_steps"] = config["num_train_epochs"] * num_update_steps_per_epoch

    lr_scheduler = get_scheduler(
        config["lr_scheduler"],
        optimizer=optimizer,
        num_warmup_steps=config["lr_warmup_steps"] * config["gradient_accumulation_steps"],
        num_training_steps=config["max_train_steps"] * config["gradient_accumulation_steps"],
    )

    controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        controlnet, optimizer, train_dataloader, lr_scheduler,
    )

    noise_scheduler = DDPMScheduler.from_pretrained(config["pretrained_model_name_or_path"], subfolder="scheduler")

    global_step = 0

    # Resume logic for ControlNet fine-tuning
    if resume_from_checkpoint is not None:
        if resume_from_checkpoint == "latest":
            checkpoints = [d for d in os.listdir(config["output_dir"]) if d.startswith("checkpoint-")] # Use output_dir
            if checkpoints:
                checkpoints.sort(key=lambda x: int(x.split('-')[1]))
                resume_from_checkpoint = os.path.join(config["output_dir"], checkpoints[-1]) # Use output_dir
            else:
                logger.warning("No ControlNet checkpoints found for resuming. Starting from scratch.")
                resume_from_checkpoint = None

        if resume_from_checkpoint and os.path.exists(resume_from_checkpoint):
            logger.info(f"Resuming ControlNet depth training from checkpoint: {resume_from_checkpoint}")
            accelerator.load_state(resume_from_checkpoint)
            global_step = int(resume_from_checkpoint.split("-")[-1])
            logger.info(f"Resumed ControlNet depth training from global_step: {global_step}")
        else:
            logger.warning(f"ControlNet resume path '{resume_from_checkpoint}' not found. Starting from scratch.")
            resume_from_checkpoint = None

    progress_bar = tqdm(
        range(global_step, config["max_train_steps"]),
        desc="ControlNet Depth Steps",
        initial=global_step,
        #total=config["max_train_steps"],
        disable=not accelerator.is_local_main_process,
    )

    for epoch in range(config["num_train_epochs"]):
        controlnet.train()
        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(controlnet):
                # Ensure latents and noise are on the correct device and dtype
                latents = vae.encode(batch["pixel_values"].to(accelerator.device, dtype=vae.dtype)).latent_dist.sample()
                latents = latents * vae.config.scaling_factor

                noise = torch.randn_like(latents, device=accelerator.device, dtype=latents.dtype) # Ensure noise is on device with correct dtype
                bsz = latents.shape[0]
                timesteps = torch.randint(
                    0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device
                ).long()

                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
                encoder_hidden_states = text_encoder(batch["input_ids"].to(accelerator.device))[0] # Ensure input_ids are on device

                down_block_res_samples, mid_block_res_sample = controlnet(
                    noisy_latents.to(controlnet.dtype), # Ensure noisy_latents has correct dtype for controlnet
                    timesteps.to(accelerator.device), # Ensure timesteps are on device
                    encoder_hidden_states.to(controlnet.dtype), # Ensure encoder_hidden_states has correct dtype for controlnet
                    controlnet_cond=batch["conditioning_pixel_values"].to(accelerator.device, dtype=controlnet.dtype), # Ensure conditioning_pixel_values are on device with correct dtype
                    return_dict=False,
                )

                model_pred = unet(
                    noisy_latents.to(unet.dtype), # Ensure noisy_latents has correct dtype for unet
                    timesteps.to(accelerator.device), # Ensure timesteps are on device
                    encoder_hidden_states.to(unet.dtype), # Ensure encoder_hidden_states has correct dtype for unet
                    down_block_additional_residuals=[sample.to(unet.dtype) for sample in down_block_res_samples], # Ensure residuals have correct dtype for unet
                    mid_block_additional_residual=mid_block_res_sample.to(unet.dtype), # Ensure residual has correct dtype for unet
                    return_dict=False,
                )[0]

                # Ensure model_pred and noise are on the same device and dtype for loss calculation
                loss = F.mse_loss(model_pred.float(), noise.float(), reduction="mean")

                # Backpropagate and update weights
                accelerator.backward(loss)
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(controlnet.parameters(), 1.0)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1

                if global_step % config["checkpointing_steps"] == 0:
                    if accelerator.is_main_process:
                        save_path = os.path.join(config["output_dir"], f"checkpoint-{global_step}")
                        accelerator.save_state(save_path)
                        logger.info(f"Saved ControlNet checkpoint to {save_path}")


            logs = {"loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0]}
            progress_bar.set_postfix(**logs)
            accelerator.log(logs, step=global_step)

            if global_step >= config["max_train_steps"]:
                break

    accelerator.wait_for_everyone()

    # --- 4. Save the Fine-tuned ControlNet Model ---
    if accelerator.is_main_process:
        controlnet = accelerator.unwrap_model(controlnet)
        controlnet.save_pretrained(config["output_dir"])
        logger.info(f"Fine-tuned ControlNet model saved to '{config['output_dir']}'")

    accelerator.end_training()

# --- Main execution block ---
if __name__ == "__main__":
    input_images_directory = config["train_data_dir"]
    annotation_images_directory = config["annotation_data_dir"]
    output_captions_directory_for_json = os.path.dirname(config["caption_file"]) # Extract dir from config path
    caption_json_path = config["caption_file"]

    # --- Run Fine-tuning ---
    print("\n--- Starting ControlNet Depth Fine-tuning ---\n")
    train_controlnet_depth(config) # Commented out as training is complete
    print("--- ControlNet Depth Fine-tuning Complete ---")





"""# Inference on COntrolnet Fine Tune"""

# --- Configuration for Simplified ControlNet Inference ---
config = {
    "pretrained_model_name_or_path": "runwayml/stable-diffusion-v1-5", # Base SD model
    "controlnet_finetuned_model_path": "Output_ControlNet_Finetune", # Path to your fine-tuned ControlNet mode

    "resolution": 512,
    "seed": 42,
    "enable_xformers_memory_efficient_attention": False, # Set to True if you installed xformers
}

# --- Setup Logging ---
logger = logging.getLogger(__name__)
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)

# --- Function for Combined Inference ---
def generate_image_with_controlnet(
    base_sd_model_path,
    controlnet_finetuned_path,
    prompt,
    control_image_path,
    output_image_path="generated_image_controlnet.png",
    num_inference_steps=30,
    guidance_scale=7.5,
    seed=None,
    resolution=512,

):
    """
    Generates an image using a base Stable Diffusion model and a fine-tuned ControlNet model.

    Args:
        base_sd_model_path (str): Path to the base Stable Diffusion model (e.g., "runwayml/stable-diffusion-v1-5").
        controlnet_finetuned_path (str): Path to the directory where your fine-tuned ControlNet model is saved.
        prompt (str): The text prompt for image generation.
        control_image_path (str): Path to the depth map (control image) to guide generation.
        output_image_path (str): Path to save the generated image.
        num_inference_steps (int): Number of denoising steps.
        guidance_scale (float): Classifier-free guidance scale.
        seed (int, optional): Random seed for reproducibility.
        resolution (int): Resolution for the generated image.
        combined_pipeline_save_path (str, optional): Path to save the combined pipeline.
    """
    print(f"\n--- Generating image with Base SD and Fine-tuned ControlNet ---")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dtype = torch.float16 if torch.cuda.is_available() else torch.float32 # Use float16 on GPU

    # Load your fine-tuned ControlNet model separately
    controlnet_model = ControlNetModel.from_pretrained(controlnet_finetuned_path, torch_dtype=dtype)

    # --- Manually construct the StableDiffusionControlNetPipeline ---
    # Load individual components of the base Stable Diffusion pipeline
    # This gives us explicit control over each part and their dtypes.
    vae = AutoencoderKL.from_pretrained(base_sd_model_path, subfolder="vae", torch_dtype=dtype)
    tokenizer = AutoTokenizer.from_pretrained(base_sd_model_path, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(base_sd_model_path, subfolder="text_encoder", torch_dtype=dtype)
    unet = UNet2DConditionModel.from_pretrained(base_sd_model_path, subfolder="unet", torch_dtype=dtype)
    scheduler = DDPMScheduler.from_pretrained(base_sd_model_path, subfolder="scheduler")
    feature_extractor = CLIPFeatureExtractor.from_pretrained(base_sd_model_path, subfolder="feature_extractor")

    # Manually construct the pipeline by passing all components, including the loaded ControlNet
    pipeline = StableDiffusionControlNetPipeline(
        vae=vae,
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        unet=unet,
        controlnet=controlnet_model, # Your fine-tuned ControlNet
        scheduler=scheduler,
        safety_checker=None, # Disable for simplicity, enable for public applications
        feature_extractor=feature_extractor,
        image_encoder=None, # Explicitly set to None if not used in this pipeline setup
        requires_safety_checker=False,
    )

    pipeline.to(device)

    #if config["enable_xformers_memory_efficient_attention"]:
        #pipeline.enable_xformers_memory_efficient_attention()

    control_image = Image.open(control_image_path).convert("RGB")
    control_image = control_image.resize((resolution, resolution), Image.LANCZOS)

    # The pipeline expects a list of PIL Images for batching, or a single PIL Image.
    # Passing a list of PIL Images lets the pipeline handle the internal tensor conversion.
    input_image_for_pipeline = [control_image]

    generator = None
    if seed is not None:
        generator = torch.Generator(device=device).manual_seed(seed)

    with torch.no_grad():
        generated_images = pipeline(
            prompt,
            image=input_image_for_pipeline,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            generator=generator,
        ).images

    generated_images[0].save(output_image_path)
    print(f"Generated image saved to '{output_image_path}'")
    print(f"Prompt: '{prompt}'")
    print(f"Control Image: '{control_image_path}'")

    # Save the combined pipeline if a save path is provided
    #if combined_pipeline_save_path:
        #print(f"Saving combined pipeline to '{combined_pipeline_save_path}'")
        #pipeline.save_pretrained(combined_pipeline_save_path)
        #print("Combined pipeline saved.")

# --- Main execution block for Simplified ControlNet Inference ---
if __name__ == "__main__":
    project_root = "/content/drive/MyDrive"

    # Corrected path to match your saved directory
    config["controlnet_finetuned_model_path"] = os.path.join(project_root, "Output_ControlNet_Finetune")

    # Ensure annotation directory exists for inference control image
    annotation_images_directory = os.path.join(project_root, "annotation")

    print(f"Project root set to: {project_root}")
    print("Ensured necessary directories exist for Combined Inference.")


    # --- Run Inference with Style-tuned SD and Depth ControlNet ---
    base_sd_model_path = config["pretrained_model_name_or_path"]
    controlnet_finetuned_path = config["controlnet_finetuned_model_path"]

    inference_prompt = "a modern living room with a large window, natural light, high quality, with the same furniture"
    inference_control_image_name = "ID_1501_depth.png"
    inference_control_image_path = os.path.join(annotation_images_directory, inference_control_image_name)
    inference_output_image = os.path.join(project_root, "generated_image_controlnet_with furniture.png")

    # Check if the fine-tuned ControlNet model exists before attempting inference
    if os.path.exists(controlnet_finetuned_path) and os.path.exists(os.path.join(controlnet_finetuned_path, "diffusion_pytorch_model.safetensors")): # Check for .safetensors
        if os.path.exists(inference_control_image_path):
            generate_image_with_controlnet(
                base_sd_model_path,
                controlnet_finetuned_path,
                inference_prompt,
                inference_control_image_path,
                output_image_path=inference_output_image,
                num_inference_steps=25,
                guidance_scale=8.0,
                seed=random.randint(0, 100000)
            )
        else:
            print(f"Cannot run inference: Control image '{inference_control_image_path}' not found.")
    else:
        print(f"Cannot run inference: Fine-tuned ControlNet model not found at '{controlnet_finetuned_path}'. "
              f"Please run 'simple_controlnet_finetune.py' first to fine-tune and save the ControlNet model.")

# Howto save base sd loaded model

"""# Saving Combined model for further use"""

import os
import random
import logging
from pathlib import Path

import torch
from PIL import Image
import numpy as np
import cv2

from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler
from transformers import AutoTokenizer, CLIPTextModel, CLIPFeatureExtractor

# --- Configuration for Simplified ControlNet Inference ---
config = {
    "pretrained_model_name_or_path": "runwayml/stable-diffusion-v1-5", # Base SD model
    "controlnet_finetuned_model_path": "Output_ControlNet_Finetune", # Path to your fine-tuned ControlNet model
    "combined_pipeline_save_path": "combined_sd_controlnet_depth_pipeline",

    "resolution": 512,
    "seed": 42,
    "enable_xformers_memory_efficient_attention": False,
}

# --- Setup Logging ---
logger = logging.getLogger(__name__)
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)

# --- Function for Combined Inference ---
def generate_image_with_controlnet(
    base_sd_model_path,
    controlnet_finetuned_path,
    prompt,
    control_image_path,
    output_image_path="generated_image_controlnet_with_furniture.png",
    num_inference_steps=30,
    guidance_scale=8.5,
    seed=None,
    resolution=512,
    combined_pipeline_save_path=None # New argument for saving/loading the full pipeline
):
    """
    Generates an image using a base Stable Diffusion model and a fine-tuned ControlNet model.
    Includes functionality to save and load the full combined pipeline.

    Args:
        base_sd_model_path (str): Path to the base Stable Diffusion model (e.g., "runwayml/stable-diffusion-v1-5").
        controlnet_finetuned_path (str): Path to the directory where your fine-tuned ControlNet model is saved.
        prompt (str): The text prompt for image generation.
        control_image_path (str): Path to the depth map (control image) to guide generation.
        output_image_path (str): Path to save the generated image.
        num_inference_steps (int): Number of denoising steps.
        guidance_scale (float): Classifier-free guidance scale.
        seed (int, optional): Random seed for reproducibility.
        resolution (int): Resolution for the generated image.
        combined_pipeline_save_path (str, optional): Path to save/load the full combined pipeline.
                                                      If None, the pipeline is not saved/loaded.
    """
    print(f"\n--- Generating image with Base SD and Fine-tuned ControlNet ---")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dtype = torch.float16 if torch.cuda.is_available() else torch.float32 # Use float16 on GPU

    pipeline = None
    if combined_pipeline_save_path and os.path.exists(combined_pipeline_save_path):
        print(f"Attempting to load combined pipeline from: {combined_pipeline_save_path}")
        try:
            pipeline = StableDiffusionControlNetPipeline.from_pretrained(
                combined_pipeline_save_path,
                torch_dtype=dtype,
                safety_checker=None,
                requires_safety_checker=False,
            )
            print("Combined pipeline loaded successfully.")
        except Exception as e:
            print(f"Failed to load combined pipeline from {combined_pipeline_save_path}: {e}")
            print("Proceeding to build pipeline from individual components.")
            pipeline = None # Reset pipeline to None to trigger building from components

    if pipeline is None:
        print("Building combined pipeline from individual components.")
        # --- Load individual components of the base Stable Diffusion pipeline ---
        vae = AutoencoderKL.from_pretrained(base_sd_model_path, subfolder="vae", torch_dtype=dtype)
        tokenizer = AutoTokenizer.from_pretrained(base_sd_model_path, subfolder="tokenizer")
        text_encoder = CLIPTextModel.from_pretrained(base_sd_model_path, subfolder="text_encoder", torch_dtype=dtype)
        unet = UNet2DConditionModel.from_pretrained(base_sd_model_path, subfolder="unet", torch_dtype=dtype)
        scheduler = DDPMScheduler.from_pretrained(base_sd_model_path, subfolder="scheduler")
        feature_extractor = CLIPFeatureExtractor.from_pretrained(base_sd_model_path, subfolder="feature_extractor")

        # Load your fine-tuned ControlNet model separately
        controlnet_model = ControlNetModel.from_pretrained(controlnet_finetuned_path, torch_dtype=dtype)

        # --- Manually construct the StableDiffusionControlNetPipeline ---
        pipeline = StableDiffusionControlNetPipeline(
            vae=vae,
            text_encoder=text_encoder,
            tokenizer=tokenizer,
            unet=unet,
            controlnet=controlnet_model, # Your fine-tuned ControlNet
            scheduler=scheduler,
            safety_checker=None,
            feature_extractor=feature_extractor,
            image_encoder=None,
            requires_safety_checker=False,
        )


        # Save the combined pipeline if a save path is provided
        if combined_pipeline_save_path:
            print(f"Saving combined pipeline to: {combined_pipeline_save_path}")
            pipeline.save_pretrained(combined_pipeline_save_path)
            print("Combined pipeline saved successfully.")


    pipeline.to(device)

    #if config["enable_xformers_memory_efficient_attention"]:
       # pipeline.enable_xformers_memory_efficient_attention()

    control_image = Image.open(control_image_path).convert("RGB")
    control_image = control_image.resize((resolution, resolution), Image.LANCZOS)

    input_image_for_pipeline = [control_image]

    generator = None
    if seed is not None:
        generator = torch.Generator(device=device).manual_seed(seed)

    with torch.no_grad():
        generated_images = pipeline(
            prompt,
            image=input_image_for_pipeline,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            generator=generator,
        ).images

    generated_images[0].save(output_image_path)
    print(f"Generated image saved to '{output_image_path}'")
    print(f"Prompt: '{prompt}'")
    print(f"Control Image: '{control_image_path}'")

# --- Main execution block for Simplified ControlNet Inference ---
if __name__ == "__main__":
    project_root = "/content/drive/MyDrive"

    # Corrected path to match your saved directory for ControlNet
    config["controlnet_finetuned_model_path"] = os.path.join(project_root, "Output_ControlNet_Finetune")
    # New: Path to save/load the combined pipeline
    config["combined_pipeline_save_path"] = os.path.join(project_root, "combined_sd_controlnet_pipeline")

    # Ensure annotation directory exists for inference control image
    annotation_images_directory = os.path.join(project_root, "annotation")

    # Ensure directory for saving combined pipeline exists
    os.makedirs(config["combined_pipeline_save_path"], exist_ok=True)

    print(f"Project root set to: {project_root}")
    print("Ensured necessary directories exist for Combined Inference.")


    # --- Run Inference with ControlNet ---
    base_sd_model_path = config["pretrained_model_name_or_path"]
    controlnet_finetuned_path = config["controlnet_finetuned_model_path"]
    combined_pipeline_save_path = config["combined_pipeline_save_path"]

    inference_prompt = "a living room in asian style with low light and minimalist furniture, high quality"
    inference_control_image_name = "ID_1502_depth.png"
    inference_control_image_path = os.path.join(annotation_images_directory, inference_control_image_name)
    inference_output_image = os.path.join(project_root, "generated_image_controlnet_1502.png")

    if os.path.exists(controlnet_finetuned_path) and os.path.exists(os.path.join(controlnet_finetuned_path, "diffusion_pytorch_model.safetensors")):
        if os.path.exists(inference_control_image_path):
            generate_image_with_controlnet(
                base_sd_model_path,
                controlnet_finetuned_path,
                inference_prompt,
                inference_control_image_path,
                output_image_path=inference_output_image,
                num_inference_steps=25,
                guidance_scale=8.0,
                seed=random.randint(0, 100000),
                combined_pipeline_save_path=combined_pipeline_save_path # Pass the save/load path
            )
        else:
            print(f"Cannot run inference: Control image '{inference_control_image_path}' not found.")
    else:
        print(f"Cannot run inference: Fine-tuned ControlNet model not found at '{controlnet_finetuned_path}'. "
              f"Please ensure 'simple_controlnet_finetune.py' has been run successfully.")









"""# Deployment Inference code"""

from google.colab import drive
drive.mount('/content/drive')

import os
import torch
from PIL import Image
import numpy as np
import cv2
import random
import gradio as gr


from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from transformers import CLIPFeatureExtractor

config = {

    "combined_pipeline_path": "combined_sd_controlnet_pipeline",
    "resolution": 512,
    "seed": 42,
}

global pipeline
pipeline = None

Project_root = "/content/drive/MyDrive"
config["combined_pipeline_path"] = os.path.join(Project_root, "combined_sd_controlnet_pipeline")

def generate_image(
    prompt,
    control_image_path,
    output_image_path,
    num_inference_steps,
    guidance_scale,
    seed = None,
    resolution = 512,
    combined_pipeline_path = config["combined_pipeline_path"],
  ):

    print(f"\n -- Generataing image using pre-saved pipeline")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dtype = torch.float16 if torch.cuda.is_available() else torch.float32

    if not os.path.exists(combined_pipeline_path):
        print(f"Error: Combined pipeline not found at '{combined_pipeline_path}'."
               f"\nPlease Ensure the path is correct")
        return

    print(f"Loading combined pipeline from:  {combined_pipeline_path}")
    global pipeline
    if pipeline is None:
        pipeline = StableDiffusionControlNetPipeline.from_pretrained(
            combined_pipeline_path,
            torch_dtype = dtype,
            safety_checker = None,
            requires_safety_checker = False,
        )

    pipeline.to(device)


    control_image = Image.open(control_image_path).convert("RGB")
    control_image = control_image.resize((resolution, resolution), Image.LANCZOS)

    input_image_for_pipeline = [control_image]

    generator = None
    if seed is not None:
      generator = torch.Generator(device= device).manual_seed(seed)


    with torch.no_grad():
      generated_images = pipeline(
          prompt,
          image = input_image_for_pipeline,
          num_inference_steps = num_inference_steps,
          guidance_scale = guidance_scale,
          generator = generator,
      ).images


    generated_images[0].save(output_image_path)
    print(f"Generated image saved to '{output_image_path}'")
    print(f"Prompt: '{prompt}'")
    print(f"Control Image: '{control_image_path}'")

iface = gr.Interface(
    fn = generate_image,
    inputs = [
        gr.Textbox(label = "Prompt", value="a high-quality photo of a modern interior design"),
        gr.Image(type="pil", label="Upload Image"),

    ],

    outputs=gr.Image(type="pil", label="Generated Image"),
    title="Stable DIffusion Controlnet interior design ",
    description="Generate images using a fine-tuned ControlNet (Depth) model. Upload a depth map and provide a text prompt."

)

if __name__ == "__main__":
        # For local testing, you can run the Gradio app directly
        iface.launch()

iface.close()

from google.colab import files
from IPython.display import  display

if __name__ == "__main__":
  Project_root = "/content/drive/MyDrive"
  config["combined_pipeline_path"] = os.path.join(Project_root, "combined_sd_controlnet_pipeline")

  uploaded = files.upload()

  if uploaded:
    uploaded_filename = list(uploaded.keys())[0]
    print(f"Uploaded file: {uploaded_filename}")

    # Define the output path for the generated image in your Google Drive
    output_image_path = os.path.join(Project_root, "saved_generated_image", uploaded_filename.replace(' ', '_'))
    print(output_image_path)

    # Ask the user for the prompt
    prompt = input("enter prompt: ")

    generate_image(
          config["combined_pipeline_path"],
          prompt,
          uploaded_filename,
          output_image_path=output_image_path,
          num_inference_steps=30,
          guidance_scale=8.5,
          seed=random.randint(0, 100000)
        )
  else:
    print("No file was uploaded.")

from IPython.display import display, Image
    display(Image(output_image_path))
    display(Image(uploaded_filename))