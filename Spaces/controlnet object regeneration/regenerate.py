# -*- coding: utf-8 -*-
"""Regenerate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qykdz_JfDzaa568aKA60VfUXDPExbwwa
"""

import gradio as gr
import torch
import torchvision.transforms as T
from transformers import AutoImageProcessor, AutoModelForDepthEstimation
from transformers import Mask2FormerImageProcessor, Mask2FormerForUniversalSegmentation
from diffusers import ControlNetModel, StableDiffusionInpaintPipeline
from PIL import Image, ImageFilter, ImageDraw
import numpy as np
import cv2
import random
from typing import List, Tuple, Dict, Union
from colors import COLOR_MAPPING_, ade_palette, COLOR_MAPPING_CATEGORY_

# --- Determine Device ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.float16 if DEVICE == "cuda" else torch.float32 # Use float16 for GPU, float32 for CPU
print(f"Using device: {DEVICE} with dtype: {DTYPE}")

print("\n---------------------------------------------------------------------\n")

# --- Model Loading (tuned for CUDA) ---
# Models are loaded once when this module is imported.
print("Loading AI models... This may take a moment.")

controlnet_seg = ControlNetModel.from_pretrained(
    "BertChristiaens/controlnet-seg-room",
    torch_dtype=DTYPE,
).to(DEVICE)
print(f"ControlNet (Segmentation) loaded to {DEVICE}.")

controlnet_depth = ControlNetModel.from_pretrained(
    "lllyasviel/control_v11f1p_sd15_depth",
    torch_dtype=DTYPE,
).to(DEVICE)
print(f"ControlNet (Depth) loaded to {DEVICE}.")

inpaint_model = StableDiffusionInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-inpainting",
    torch_dtype=DTYPE,
).to(DEVICE)
print(f"Stable Diffusion Inpainting Pipeline loaded to {DEVICE}.")

processor = Mask2FormerImageProcessor.from_pretrained("facebook/mask2former-swin-large-ade-semantic")
model = Mask2FormerForUniversalSegmentation.from_pretrained("facebook/mask2former-swin-large-ade-semantic")
model = model.to(DEVICE)
print(f"Mask2Former (Semantic Segmentation) loaded to {DEVICE}.")

depth_image_processor = AutoImageProcessor.from_pretrained("LiheYoung/depth-anything-large-hf", torch_dtype=DTYPE)
depth_model = AutoModelForDepthEstimation.from_pretrained("LiheYoung/depth-anything-large-hf", torch_dtype=DTYPE)
depth_model = depth_model.to(DEVICE)
print(f"Depth Estimation Model loaded to {DEVICE}.")
print("All AI models loaded successfully.")

# --- Color Mappings and Utility Functions ---



def to_rgb(color: str) -> Tuple[int, int, int]:
    """Converts a hex color string to an RGB tuple."""
    return tuple(int(color[i:i+2], 16) for i in (1, 3, 5))

def map_colors_rgb(color: Tuple[int, int, int]) -> str:
    """Maps an RGB color tuple to a semantic object name."""
    # Initialize COLOR_MAPPING_RGB if not already done (should be done once at module load)
    global COLOR_MAPPING_RGB
    if 'COLOR_MAPPING_RGB' not in globals():
        COLOR_MAPPING_RGB = {to_rgb(k): v for k, v in COLOR_MAPPING_.items()}

    if color in COLOR_MAPPING_RGB:
        return COLOR_MAPPING_RGB[color]
    else:
        # Fallback to finding the closest color name if exact match not found
        closest_color_name = "unknown"
        min_dist = float('inf')
        for mapped_rgb, name in COLOR_MAPPING_RGB.items():
            dist = np.sum((np.array(color) - np.array(mapped_rgb))**2)
            if dist < min_dist:
                min_dist = dist
                closest_color_name = name
        return closest_color_name

# Initialize COLOR_MAPPING_RGB here so it's ready when imported
COLOR_MAPPING_RGB = {to_rgb(k): v for k, v in COLOR_MAPPING_.items()}


# --- Global State for this specific app ---
_current_input_image_pil: Union[Image.Image, None] = None
_current_segmentation_np: Union[np.ndarray, None] = None
_current_segment_items_map: Dict[Tuple[int, int, int], str] = {}
_current_negative_prompt: str = "lowres, watermark, banner, logo, watermark, contactinfo, text, deformed, blurry, blur, out of focus, out of frame, surreal, ugly"

"""# Main Logic"""

def load_and_preprocess_image(image: Image.Image) -> Image.Image:
    image = image.convert("RGB")
    image = image.resize((512, 512))
    return image

def get_segmentation_data(image: Image.Image) -> Tuple[np.ndarray, Image.Image, List[str]]:
    """
    Performs semantic segmentation on the input image.
    Updates global state (_current_input_image_pil, _current_segmentation_np, _current_segment_items_map)
    and returns the raw segmentation map (numpy), a colored segmentation image (PIL),
    and a list of detected object names.
    """
    # global _current_segment_items_map, _current_segmentation_np, _current_input_image_pil

    # Ensure image is valid before processing
    if not isinstance(image, Image.Image):
        raise TypeError("Input 'image' must be a PIL Image object.")

    with torch.inference_mode():
        semantic_inputs = processor(images=image, return_tensors="pt", size={"height": 256, "width": 256})
        semantic_inputs = {key: value.to(DEVICE) for key, value in semantic_inputs.items()}

        semantic_outputs = model(**semantic_inputs)

        if hasattr(semantic_outputs, 'logits') and torch.is_tensor(semantic_outputs.logits):
            semantic_outputs.logits = semantic_outputs.logits.to("cpu")
        if hasattr(semantic_outputs, 'pred_masks') and torch.is_tensor(semantic_outputs.pred_masks):
            semantic_outputs.pred_masks = semantic_outputs.pred_masks.to("cpu")

        segmentation_maps = processor.post_process_semantic_segmentation(semantic_outputs, target_sizes=[_current_input_image_pil.size[::-1]])
        predicted_semantic_map_np = segmentation_maps[0].cpu().numpy()


    if predicted_semantic_map_np.size == 0:
        # Handle case where no objects are detected by Mask2Former
        print("Warning: Mask2Former detected no objects in the image.")
        color_seg = np.zeros((_current_input_image_pil.size[1], _current_input_image_pil.size[0], 3), dtype=np.uint8)
        detected_items = []
        temp_segment_map = {}

    else:
        color_seg = np.zeros((predicted_semantic_map_np.shape[0], predicted_semantic_map_np.shape[1], 3), dtype=np.uint8)
        palette = np.array(ade_palette())
        unique_labels = np.unique(predicted_semantic_map_np)


        detected_items = []
        temp_segment_map = {}

        for label in unique_labels:
            color = palette[label]
            item_name = map_colors_rgb(tuple(color))
            color_seg[predicted_semantic_map_np == label, :] = color
            if item_name not in detected_items:
                detected_items.append(item_name)
            temp_segment_map[tuple(color)] = item_name
    # _current_segment_items_map = temp_segment_map
    # _current_segmentation_np = color_seg

    seg_image = Image.fromarray(color_seg).convert('RGB')
    print(f"Detected objects for checkboxes: {detected_items}")
    return predicted_semantic_map_np, seg_image, detected_items, temp_segment_map #_current_segmentation_np, seg_image, detected_items

def _create_inpainting_mask_from_selection(
    items_to_regenerate: List[str],
    segmentation_np: np.ndarray,
    segment_items_map: Dict[Tuple[int, int, int], str],
    input_image_size: Tuple[int, int]
) -> Image.Image:
    """
    Internal helper to create a grayscale inpainting mask based on selected semantic items.
    Requires _current_segmentation_np and _current_segment_items_map to be populated.
    """
    if segmentation_np is None or segmentation_np.size == 0:
        print("Warning: Segmentation data is missing or empty. Creating a blank mask.")
        mask = np.zeros((input_image_size[1], input_image_size[0]), dtype=np.uint8)
    else:
        # Ensure segmentation_np has 3 dimensions for color comparison
        if segmentation_np.ndim == 2:
             # Convert 2D semantic map to 3D color representation
             color_seg = np.zeros((segmentation_np.shape[0], segmentation_np.shape[1], 3), dtype=np.uint8)
             palette = np.array(ade_palette())
             for label in np.unique(segmentation_np):
                 color = palette[label]
                 color_seg[segmentation_np == label, :] = color
             segmentation_np_colored = color_seg
        else:
             segmentation_np_colored = segmentation_np

        mask = np.zeros(segmentation_np_colored.shape[:2], dtype=np.uint8) # Create a 2D grayscale mask


    if items_to_regenerate:
        for color_rgb_tuple, item_name in segment_items_map.items():
            if item_name in items_to_regenerate:
                # Compare against the 3D colored segmentation map
                color_matches = (segmentation_np_colored == np.array(color_rgb_tuple)).all(axis=2)
                mask[color_matches] = 255# Set these pixels to white (masked)

    if mask.size == 0:
        print("Warning: Mask is still empty after selection. This might result in no changes.")
        mask = np.zeros((input_image_size[1], input_image_size[0]), dtype=np.uint8)


    mask_image = Image.fromarray(mask).convert("L") # Convert to grayscale PIL Image

    # Apply filters to the mask for smoother results
    mask_image = mask_image.filter(ImageFilter.MaxFilter(25)) # Expands mask slightly
    mask_image = mask_image.filter(ImageFilter.GaussianBlur(radius=5)) # Blurs the edges
    return mask_image

def _get_depth_image(image: Image.Image) -> Image.Image:
    """
    Internal helper to generate a depth map from the input image.
    """
    print("depth image processor strat")
    inputs = depth_image_processor(images=image, return_tensors="pt")
    image_to_depth = {key: value.to(DEVICE).to(DTYPE) for key, value in inputs.items()} # Cast to model's dtype
    print("depth image processor end")

    print("_____________")
    print("depth model")
    with torch.no_grad():
        if DEVICE == "cuda" and DTYPE == torch.float16:
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                print("EPTH map continuous (autocast enabled)")
                depth_map = depth_model(**image_to_depth).predicted_depth
                print("depth map continous 2")
        else:
            print("EPTH map continuous (autocast not enabled)")
            depth_map = depth_model(**image_to_depth).predicted_depth
            print("depth map continous 2")

    depth_map = depth_map.to("cpu")
    print("depth model end")
    print("_____________")

    print("functional interplot start")
    # REPLACE THE ABOVE CODE WITH THIS
    width, height = image.size
    # Step 1: Convert to float32 specifically for interpolation
    depth_map_float32 = depth_map.unsqueeze(1)

    # Step 2: Perform the interpolation
    interpolated_map = torch.nn.functional.interpolate(
        depth_map_float32,
        size=(height, width),
        mode="bicubic",
        align_corners=False,
    )
    print("functional interplot end")

    # Step 3: IMPORTANT - Convert back to the model's original DTYPE
    #depth_map = interpolated_map.to(DTYPE)
    print("functiona")
    depth_min = torch.amin(interpolated_map, dim=[1, 2, 3], keepdim=True)
    depth_max = torch.amax(interpolated_map, dim=[1, 2, 3], keepdim=True)
    depth_map_normalized = (interpolated_map - depth_min) / (depth_max - depth_min)


    # Concatenate for RGB channels
    image_array = torch.cat([depth_map_normalized] * 3, dim=1)

    # Squeeze the batch dimension and permute to (H, W, C) for numpy conversion
    image_array = image_array.squeeze(0).permute(1, 2, 0).cpu().numpy()

    # Convert the numpy array to a PIL Image
    image = Image.fromarray((image_array * 255.0).clip(0, 255).astype(np.uint8))
    return image

def make_image_controlnet(
    image: Image.Image,
    mask_image: Image.Image,
    controlnet_conditioning_image: Union[Image.Image, np.ndarray],
    positive_prompt: str,
    negative_prompt: str,
    strength: float,
    seed: int ) -> Image.Image:

        """
        Main function to regenerate selected objects in the last processed image.

        Args:
            prompt (str): The text prompt describing the new objects.
            items_to_regenerate (List[str]): A list of object names (e.g., "wall", "bed")
                                            to be regenerated. These names come from the
                                            list returned by get_segmentation_data.
            strength (float): ControlNet strength (0.0 to 1.0). Higher means more adherence
                              to ControlNet guidance.
            seed (int): Random seed for reproducibility.

        Returns:
            PIL.Image.Image: The regenerated image.
        """
        if _current_input_image_pil is None:
            raise ValueError("No image has been processed yet. Call get_segmentation_data(your_image) first.")
        if _current_segmentation_np is None:
            raise ValueError("Segmentation data not available. Call get_segmentation_data(your_image) first.")
        if not positive_prompt:
            raise ValueError("A prompt is required for regeneration.")

        generator = torch.Generator(device=DEVICE).manual_seed(seed)

        image_512 = image.resize((512, 512))
        mask_image_512 = mask_image.resize((512, 512))

        control_nets = [controlnet_seg, controlnet_depth]

        # # Ensure all input images for the pipeline are of the expected size (512x512)
        # base_image_for_pipeline = _current_input_image_pil.resize((512, 512))
        # inpainting_mask_for_pipeline = inpainting_mask.resize((512, 512))
        # depth_mask_for_pipeline = depth_mask.resize((512, 512))

        if isinstance(controlnet_conditioning_image, np.ndarray):
            # Convert numpy array to PIL Image, ensuring it's in a compatible format (e.g., uint8)
            seg_map_pil = Image.fromarray(controlnet_conditioning_image.astype(np.uint8)).convert("RGB")
        else:
            seg_map_pil = controlnet_conditioning_image.convert("RGB")

        seg_map_512 = seg_map_pil.resize((512, 512))
        depth_map_512 = _get_depth_image(image).resize((512, 512))



        # Convert PIL Images to tensors and ensure they are of the correct DTYPE
        control_conditioning_tensors = [
            T.ToTensor()(seg_map_512).to(DEVICE).to(DTYPE),
            T.ToTensor()(depth_map_512).to(DEVICE).to(DTYPE)
        ]

        generated_image = inpaint_model(
            prompt=positive_prompt,
            negative_prompt=negative_prompt,
            image=image_512,
            mask_image=mask_image_512,
            controlnet=control_nets,
            controlnet_conditioning_image=control_conditioning_tensors,
            num_inference_steps=50,
            strength=strength,
            generator=generator
        ).images[0]

        print("Image regeneration complete.")
        return generated_image

"""# Gradio Functions"""

# --- Gradio UI Event Handlers ---

def on_upload_image(input_image_upload: Image.Image) -> Tuple[gr.Image, gr.Radio, gr.CheckboxGroup, gr.Textbox, gr.Textbox,  gr.Button]:
    """
    Handles the initial image upload and processing.
    Corresponds to Streamlit's `on_upload` and initial processing logic.
    """
    global _current_input_image_pil, _current_segmentation_np, _current_segment_items_map

    if input_image_upload is None:
        # Reset all relevant UI elements if no image is uploaded
        return (
            gr.update(value=None), # Clear image
            gr.update(choices=[], value=[], interactive=False), # regenerate_objects_checkboxes
            gr.update(value="", interactive=False), # positive_prompt_textbox
            gr.update(interactive=False)  # generate_button
        )

    try:
        _current_input_image_pil = load_and_preprocess_image(input_image_upload)
        segmentation_np, _, detected_objects, segment_items_map = get_segmentation_data(_current_input_image_pil)

        _current_segmentation_np = segmentation_np
        _current_segment_items_map = segment_items_map
        formatted_choices = [(name, name) for name in detected_objects]


        return (
            gr.update(value=_current_input_image_pil), # Display uploaded image
            gr.update(choices=formatted_choices, value=[c[0] for c in formatted_choices], interactive=True), # Populate and enable checkboxes
            gr.update(value="", interactive=True), # Enable positive prompt
            gr.update(interactive=True)  # Enable generate button
        )
    except Exception as e:
        print(f"Error during image processing: {e}")
        gr.Warning(f"Failed to process image: {e}")
        return (
            gr.update(value=None),
            gr.update(choices=[], value=[], interactive=False),
            gr.update(value="", interactive=False),
            gr.update(interactive=False)
        )

def handle_generate_button(
   positive_prompt: str,
    chosen_colors_names: List[str], # This now receives the list of chosen object names (strings)
) -> Tuple[gr.Image, gr.Button]:
    """
    Handles the main 'Generate Output' button click for Regenerate mode.
    """
    global _current_input_image_pil, _current_segmentation_np, _current_segment_items_map , _current_negative_prompt

    if _current_input_image_pil is None:
        gr.Error("Please upload and process an image first.")
        return None, gr.update(interactive=False)

    if not positive_prompt:
        gr.Error("Positive prompt is required for 'Regenerate' mode.")
        return None, gr.update(interactive=False)
    try:
        print(f"Selected items for regeneration: {chosen_colors_names}")
        if _current_segmentation_np is None:
            print("seg")
            gr.Error("Segmentation data is not available. Please re-process the image.")
            return None, gr.update(interactive=False)

        inpainting_mask = _create_inpainting_mask_from_selection(
             chosen_colors_names, _current_segmentation_np, _current_segment_items_map, _current_input_image_pil.size
         )
        current_seed = random.randint(0, 2**32)

        result_image = make_image_controlnet(
            image=_current_input_image_pil,
            mask_image=inpainting_mask,
            controlnet_conditioning_image=_current_segmentation_np,
            positive_prompt=positive_prompt,
            negative_prompt=_current_negative_prompt,
            strength=0.85,
            seed=current_seed
        )
        print("Regeneration done")


    except Exception as e:
        print(f"Error during Regeneration: {e}")
        gr.Error(f"Error during Regeneration: {e}")
        return None, gr.update(interactive=False)

    # move_output_btn_state = gr.update(interactive=result_image is not None)
    return result_image #, move_output_btn_state



